{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_id = os.environ['TEST_PROJECT_ID']\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from dataproc.client import DataProc\n",
    "from cloud_storage.client import CloudStorage\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'europe-west1'\n",
    "zone_letter = 'd'\n",
    "bucket_name = 'letter_statistics_example'\n",
    "\n",
    "\n",
    "dp = DataProc(project_id, region, zone_letter)\n",
    "cs = CloudStorage(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the lines that comes out of this and insert it in the cell below\n",
    "print(os.path.join(os.environ['HOME'], 'gcp/dataproc/examples/letter_statistics/data_generator.py'))\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i [YOUR_DATA_GENERATOR_PATH] --project_id=[YOUR_PROJECT_ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a cluster\n",
    "\n",
    "First upload an initialization script for the cluster. Afterwards create the cluster with the just uploaded initialization script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = os.path.join(os.environ['HOME'], 'gcp/dataproc/examples/letter_statistics/initialize_cluster.sh')\n",
    "initialization_file = 'gs://{}/initialize_cluster.sh'.format(bucket_name)\n",
    "\n",
    "cs.upload_blob_from_filename(source_file, initialization_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = 'cluster-test'\n",
    "master_machine_type = 'n1-standard-1'\n",
    "nr_masters = 1\n",
    "master_boot_disk_gb = 200\n",
    "worker_machine_type = 'n1-standard-1'\n",
    "nr_workers = 2\n",
    "worker_boot_disk_gb = 100\n",
    "metadata = {'MINICONDA_VARIANT': '3', 'MINICONDA_VERSION': '4.5.11'}\n",
    "\n",
    "dp.create_cluster(cluster_name,\n",
    "                  master_machine_type,\n",
    "                  nr_masters,\n",
    "                  master_boot_disk_gb,\n",
    "                  worker_machine_type,\n",
    "                  nr_workers,\n",
    "                  worker_boot_disk_gb,\n",
    "                  metadata,\n",
    "                  initialization_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit a pyspark job\n",
    "\n",
    "Starting up a dataproc cluster can take a few minutes. So be patient with the next steps. If you trigger the job before the cluster has been fully initialized, then the cluster initialization may fail and your job may fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = os.path.join(os.environ['HOME'], 'gcp/dataproc/examples/letter_statistics/calculate_letter_statistics.py')\n",
    "main_python_file = 'gs://{}/calculate_letter_statistics.py'.format(bucket_name)\n",
    "\n",
    "cs.upload_blob_from_filename(source_file, main_python_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'gs://{}/letter_files/outputs/letter_statistics.csv'.format(bucket_name)\n",
    "\n",
    "script_parameters = ['--project_id={}'.format(project_id),\n",
    "                     '--input_file_glob=gs://{}/data/inputs/*'.format(bucket_name),\n",
    "                     '--output_path={}'.format(output_path)]\n",
    "\n",
    "job_id = dp.submit_pyspark_job(cluster_name, main_python_file, script_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.wait_for_job(job_id, request_interval=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results\n",
    "\n",
    "Since the job writes the statistics to cloud storage, let's read the output and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_content = cs.get_blob_content(output_path)\n",
    "df = pd.read_csv(StringIO(string_content), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "\n",
    "ax.set_xticks(range(len(df.index.values)))\n",
    "ax.set_xticklabels(df.index)\n",
    "df[['MEAN', 'VARIANCE', 'MIN', 'MAX']].plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup: delete cluster and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.delete_cluster(cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.delete_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dataproc",
   "language": "python",
   "name": "dataproc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
